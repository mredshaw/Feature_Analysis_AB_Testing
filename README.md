# Feature Analysis and A/B Testing

## Project Overview
This project involves conducting statistical analysis and A/B testing on feature variants for a mobile application. The objective is to determine which feature variant results in better user engagement metrics such as Click-Through-Rate (CTR) and Average Time Spent.

## Scenario
As a Product Analyst for a mobile application, you are testing changes to an existing feature. After running an experiment for 2 weeks, you have collected data on CTR and Average Time Spent for two feature variants. Your task includes:

1. Analyzing the results to determine which feature (if any) results in CTR or Time Spent lift.
2. Conducting statistical testing to determine if there is a statistically significant difference between the features and the control group.
3. Summarizing your results and making a recommendation to the engineering team about which feature to deploy.
4. Creating a roll-out plan for introducing the feature to your audience.

## Contents
- `Data_Science_A2_FINAL.ipynb`: Jupyter Notebook containing the code for analysis and statistical testing, as well as markdown cells for summarizing results and the roll-out plan.
- `data/`: Folder containing the dataset used for analysis (if applicable).

## Tools and Libraries Used
- Python
- Pandas
- NumPy
- SciPy
- Matplotlib
- Seaborn

## How to Run
1. Clone this repository: `git clone https://github.com/mredshaw/Feature_Analysis_AB_Testing.git`
2. Open the Jupyter Notebook: `Data_Science_A2_FINAL.ipynb`
3. Execute the cells in the notebook to see the analysis and results.

## Results and Recommendations
The detailed results, analysis, and recommendations are provided within the Jupyter Notebook. These include the statistical significance of the differences between feature variants and the control group, and a proposed roll-out plan for the selected feature.
